\section{Problema 8}
\subsection{Enunciat}
Consider the following randomized algorithm.
\begin{algorithm}
\caption{}
\begin{algorithmic}[1]
\Function{RWVC}{$G=\{V,E\}$} \Comment{$G$:weighted graph}
\State $U$ = $\emptyset$;
\While{$E \neq \emptyset$}
    \State{Select an edge $e = (v,t)$};
    \State{Randomly choose $x$ from $\{v, t\}$ with $P[x=v]=\frac{w(t)}{w(v)+w(t)}$};
    \State{$U = U \cup \{x\}$};
    \State{$E = E - \{e | x \text{ is an end-point of } e\}$}
\EndWhile
\State{\Return $U$}
\EndFunction
\end{algorithmic}
\end{algorithm}

Prove that \textbf{RWVC} is a randomized 2-approximation for \textbf{Minimum weighted vertex cover} problem.
\subsection{Solució}

Per veure que \textbf{RWVC} es una 2-aproximació randomitzada de \textbf{Minimum weighted vertex cover} problem hem de demostrar el següent:
\[
E[w(s)] \leq 2E[w(s^{*})]
\]
On:
\begin{itemize}
    \item $s^{*}$ és la solució òptima
    \item $s$ és la solució obtinguda amb \textbf{RWVC}
    \item $s_i$ és la solució parcial obtinguda a la iteració $i$
    \item $p$ és el conjunt que té els elements que es troben tant a la solució òptima com a la solució parcial. Es a dir: $p=s \cap s^{*}$. $p_i$ és el mateix però utilitzant $s_i$.
    \item $r$ és el conjunt que té els elements que es troben a la solució parcial però no a la solució òptima. Es a dir: $r=s \setminus s^{*}$. $r_i$ és el mateix però utilitzant $s_i$.
    \item $w(x)$ és la suma total de pesos al conjunt $x$
    \item $x_i^{inc}$ és l'increment de pes a la iteració $i$ al conjunt $x$. Es a dir: $x_i^{inc} = w(x_i) - w(x_{i-1})$
    
\end{itemize}
\newpage
\subsubsection{Demostració}
Per començar definirem el següent lema:
\par
\textbf{Lema b}: L'esperança de la suma dels pesos que apareixen tant a $s$ com a $s^{*}$, és més gran que l'esperança de la suma dels pesos dels elements que apareixen a $s$ però no a $s^{*}$. Per tant:
\[
E[w(p)] \geq E[w(r)]
\]
\textbf{Demostració lema b:}
\newline
Per fer-ho, pensarem en què passa al afegir un vèrtex a $s_i$. Sabem que el vèrtex és un extrem de la aresta $(v,t)$, la qual no està coberta. Per tant,  $s^{*}$ ha de contenir almenys un dels dos vèrtexs:
\begin{eqnarray}
    w(p_i) \geq \min_{\{w(v), w(t)\}}+w(p_{i-1}) \\
    E[w(v)] = w(v)*P[x=v]=\frac{w(v)w(t)}{w(v)+w(t)} = w(t)*(1-P[x=v]) = E[w(t)] \\
    E[p_i^{inc}] \geq \frac{w(v)*w(t)}{w(v)+w(t)}
\end{eqnarray}
\begin{itemize}
    \item (4) El pes de $p_i$ és més gran o igual que el pes mínim de $v$ o $t$ més el pes que ja teníem ($p_{i-1}$), ja que almenys afegirem un dels dos. 
    \item (5) L'esperança del pes de $v$ és igual a l'esperança del pes de $t$
    \item (6) Per tant, per (4) i (5), l'increment mínim a l'esperança de $p_i$ és l'esperança del pes de $v$ o $t$, que és el mateix.
\end{itemize}
Degut a això, com que afegim un dels vèrtexs de $s^{*}$ a $s_i$, a $r$ només podem incloure un dels dos com a màxim (l'altre vèrtex o cap). Per tant:
\begin{eqnarray}
    w(r_i) \geq \max_{\{w(v), w(t)\}}+w(r_{i-1}) \\
    E[r_i^{inc}] \leq \frac{w(v)*w(t)}{w(v)+w(t)}
\end{eqnarray}
\begin{itemize}
    \item (7) El pes de $r_i$ és com a molt el pes que ja teníem més el màxim del pes de $v$ o $t$.
    \item (8) Com hem vist a (5), l'esperança del pes de $v$ i $t$ es la mateixa, i com que sabem com a molt inclourem només un vèrtex, l'increment a l'esperança de $r_i$ és més petit o igual a aquesta esperança.
\end{itemize}
Per tant, per (6) i (8), tenim:
\[
E[w(p_i^{inc})] \geq \frac{w(v)w(t)}{w(v)+w(t)} \geq E[w(r_i^{inc})]
\]
Com que aquesta desigualtat es compleix per tot $i$, desprès de realitzar els $i$ increments sabem que:
\[
E[w(p)] \geq E[w(r)]
\]
\renewcommand\qedsymbol{$\square$}
\qed
\newpage
Degut al \textbf{lema b} sabem que:
\begin{eqnarray}
    E[w(p)] \geq E[w(r)]\nonumber \\
    = E[w(s \cap s^{*})] \geq E[w(s \setminus s^{*})] \nonumber \\
    E[w(s^{*})] \geq E[w(s)] - E[w(s^{*})]  \\
    \Rightarrow 2E[w(s^{*})] \geq E[w(s)] \nonumber
\end{eqnarray}
\begin{itemize}
    \item (9) Ja que $s \cap s^{*} \subseteq s^{*} \Rightarrow w(s \cap s^{*}) \leq w(s^{*})$ i $w(s \setminus s^{*}) = w(s) - w(s^{*})$.
\end{itemize}
\renewcommand\qedsymbol{$\blacksquare$}
\qed